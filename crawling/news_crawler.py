# 1. ìš”ì¦˜ IT
from selenium import webdriver
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
import kss
import requests
from bs4 import BeautifulSoup
from datetime import date, timedelta,datetime


SOURCE = "YOZM_IT" 
BASE_URL = "https://yozm.wishket.com"

MODEL_NAME = "skt/kobert-base-v1"
device = torch.device("cpu")
yesterday = date.today() - timedelta(days=1)
yesterday_format = yesterday.strftime("%Y.%m.%d")
today = date.today() 
today_format = today.strftime("%Y.%m.%d")

# 1) ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
print("â–¶ KoBERT ëª¨ë¸ ë¡œë“œ ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME)
model.to(device)
model.eval()
print("â–¶ ë¡œë“œ ì™„ë£Œ!")

# 2) í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬
def split_sentences_kor(text: str):
    """
    ê¸´ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬.
    kssë¥¼ ì‚¬ìš©í•´ì„œ ì•ˆì „í•˜ê²Œ ë¬¸ì¥ ë¶„ë¦¬.
    """
    sentences = kss.split_sentences(text)
    sentences = [s.strip() for s in sentences if s.strip()]
    return sentences


# 3) ë¬¸ì¥ë“¤ì„ KoBERT CLS ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
def encode_sentences(sentences, batch_size: int = 8, max_length: int = 256):
    """
    ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ -> CLS ì„ë² ë”© (numpy array: [num_sent, hidden_dim])
    """
    all_embeddings = []

    with torch.no_grad():
        for i in range(0, len(sentences), batch_size):
            batch = sentences[i:i+batch_size]

            enc = tokenizer(
                batch,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt"
            )

            # ğŸ” ë””ë²„ê·¸: input_ids ë²”ìœ„ í™•ì¸
            ids = enc["input_ids"]
            #print("min id:", ids.min().item(), "max id:", ids.max().item())

            # ğŸ”¥ ì¤‘ìš”: token_type_ids ê°•ì œë¡œ ì œê±°
            if "token_type_ids" in enc:
                #print("-> token_type_ids ì œê±°")
                enc.pop("token_type_ids")

            enc = {k: v.to(device) for k, v in enc.items()}

            outputs = model(**enc)
            # BERTì˜ [CLS] í† í° ë²¡í„° ì‚¬ìš©
            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [batch, hidden]
            all_embeddings.append(cls_embeddings.cpu().numpy())

    return np.vstack(all_embeddings)  # [num_sent, hidden_dim]


# 4) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ì¤‘ìš”í•œ ë¬¸ì¥ top_k ì¶”ì¶œ
def summarize_kobert(text: str, top_k: int = 5):
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼
    1) ë¬¸ì¥ ë¶„ë¦¬
    2) ê° ë¬¸ì¥ì„ KoBERTë¡œ ì„ë² ë”©
    3) ë¬¸ì¥ ì„ë² ë”© vs ë¬¸ì„œ í‰ê·  ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    4) ìœ ì‚¬ë„ê°€ í° ìƒìœ„ top_k ë¬¸ì¥ì„ ì›ë˜ ìˆœì„œëŒ€ë¡œ ë½‘ê¸°
    """
    sentences = split_sentences_kor(text)

    if len(sentences) == 0:
        return "", []

    # ë¬¸ì¥ ìˆ˜ê°€ top_kë³´ë‹¤ ì ìœ¼ë©´ ê·¸ëƒ¥ ì „ì²´ ë°˜í™˜
    if len(sentences) <= top_k:
        summary = " ".join(sentences)
        return summary, sentences

    print(f"â–¶ ë¬¸ì¥ ê°œìˆ˜: {len(sentences)}ê°œ")
    sent_embs = encode_sentences(sentences)  # [N, D]

    # ë¬¸ì„œ ì„ë² ë”© = ë¬¸ì¥ ì„ë² ë”© í‰ê· 
    doc_emb = sent_embs.mean(axis=0, keepdims=True)  # [1, D]

    # L2 ì •ê·œí™” í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    def l2norm(x, axis):
        return x / (np.linalg.norm(x, axis=axis, keepdims=True) + 1e-8)

    sent_norm = l2norm(sent_embs, axis=1)  # [N, D]
    doc_norm = l2norm(doc_emb, axis=1)     # [1, D]

    sims = (sent_norm @ doc_norm.T).squeeze(1)  # [N]

    # ìœ ì‚¬ë„ ë†’ì€ ìƒìœ„ top_k ë¬¸ì¥ ì¸ë±ìŠ¤
    top_idx = sims.argsort()[::-1][:top_k]
    # ë¬¸ì„œ ì›ë˜ ìˆœì„œ ìœ ì§€ (ìš”ì•½ë¬¸ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì½íˆë„ë¡)
    top_idx_sorted = sorted(top_idx)

    summary_sentences = [sentences[i] for i in top_idx_sorted]
    summary_text = " ".join(summary_sentences)

    return summary_text, summary_sentences

def crawl_news():
    options = webdriver.ChromeOptions()
    options.add_argument("--start-maximized")
    driver = webdriver.Chrome(options=options)

    provider = "yozmIT"
    url = "https://yozm.wishket.com/magazine/list/new/"
    driver.get(url)
    wait = WebDriverWait(driver, 10)

    # í˜ì´ì§€ ë¡œë”© ê¸°ë‹¤ë¦¬ê¸° (ì ë‹¹í•œ ìƒìœ„ ì—˜ë¦¬ë¨¼íŠ¸ ê¸°ì¤€ìœ¼ë¡œ)
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "article")))

    articles = driver.find_elements(By.CSS_SELECTOR, "article")

    results = []

    for art in articles:
        try:

            # ì´ article ì•ˆì— '1ì¼ ì „'ì´ ìˆëŠ”ì§€ í™•ì¸
            date_el = art.find_element(
                By.XPATH,
                ".//span[contains(normalize-space(.), '1ì¼ ì „') or contains(normalize-space(.), 'ì‹œê°„ ì „')]"
            )
        except:
            # ì´ ì¹´ë“œì—ëŠ” '1ì¼ ì „'ì´ ì—†ìŒ â†’ ìŠ¤í‚µ
            continue

        # ì œëª©
        title_el = art.find_element(By.TAG_NAME, "h3")
        title = title_el.text.strip()

        # ë‚ ì§œ (ì—¬ê¸°ì„œëŠ” '1ì¼ ì „'ì¼ ê²ƒ)
        date_text = date_el.text.strip()

        # ë§í¬ (ì¹´ë“œ ì „ì²´ë¥¼ ê°ì‹¸ëŠ” a íƒœê·¸)
        link_el = art.find_element(
            By.XPATH,
            ".//a[@data-testid='contentsItem-item-link']"
        )
        href = link_el.get_attribute("href")
        article_id = href.rstrip("/").split("/")[-1]
        thumbnail_url = None
        try:
            # column ìŠ¤íƒ€ì¼ ì¹´ë“œì— í•´ë‹¹
            img_el = art.find_element(
                By.XPATH,
                ".//div[@data-testid='article-column-item--image']//img"
            )
            thumbnail_url = img_el.get_attribute("src")
        except Exception:
            # ìœ„ êµ¬ì¡°ê°€ ì—†ë‹¤ë©´, ì¹´ë“œ ì•ˆì˜ object-cover ì´ë¯¸ì§€ë¥¼ fallbackìœ¼ë¡œ ì‚¬ìš©
            try:
                img_el = art.find_element(
                    By.XPATH,
                    ".//img[contains(@class, 'object-cover')]"
                )
                thumbnail_url = img_el.get_attribute("src")
            except Exception:
                thumbnail_url = None  # ì •ë§ ì—†ìœ¼ë©´ None

        results.append({
            "source": SOURCE,
            "externalId": article_id,
            "title": title,
            "date": date_text,
            "url": href,
            "provider" : provider,
            'thumbnail_url' : thumbnail_url

        })

    print(results)
    detail_results = []

    for item in results:   # list_resultsëŠ” ì•ì—ì„œ ëª¨ì•„ë‘” {link, thumbnail, ...}
        driver.get(item["url"])
        def parse_article_detail(driver, timeout=15):
            data = {}
            local_wait = WebDriverWait(driver, timeout)

            # 1) ì œëª©
            # <h1 class="... typo-title3 desktop:typo-title2">...</h1>
            title_el = driver.find_element(
                By.CSS_SELECTOR,
                "h1.typo-title3, h1.typo-title2"
            )
            data["title"] = title_el.text.strip()

            # 2) ê¸€ì“´ì´ (ì‘ì„±ì)
            # <span ... data-testid="contents-author-name">FEConf</span>
            author_el = driver.find_element(
                By.CSS_SELECTOR,
                "span[data-testid='contents-author-name']"
            )
            data["author"] = author_el.text.strip()

            # 3) ê²Œì‹œ ë‚ ì§œ (ìƒëŒ€ ì‹œê°„: '1ì¼ ì „')
            # <span class="... typo-body2 ...">1ì¼ ì „</span>
            date_el = driver.find_element(
                By.XPATH,
                "//span[contains(@class, 'typo-body2') and (contains(normalize-space(.), '1ì¼ ì „') or contains(normalize-space(.), 'ì‹œê°„ ì „'))]"
            )
            a = date_el.text.strip()
            if a == '1ì¼ ì „':
                data["posted_at"] = yesterday_format  # ì˜ˆ: '1ì¼ ì „'
            else:
                data["posted_at"] = today_format

            # 4) ì¹´í…Œê³ ë¦¬
            # <a data-testid="category-link" ...><span ...>ê°œë°œ</span></a>
            category_el = driver.find_element(
                By.CSS_SELECTOR,
                "a[data-testid='category-link'] span"
            )
            data["category"] = category_el.text.strip()   # ì˜ˆ: 'ê°œë°œ'

            # 5) ë³¸ë¬¸ ë‚´ìš©
            # <section id="article-detail-wrapper"> ... ì—¬ê¸° ì•ˆì˜ p, h3, h4, blockquote ë“± ì „ì²´ í…ìŠ¤íŠ¸
            content_section = local_wait.until(EC.presence_of_element_located((
                By.CSS_SELECTOR,
                "section#article-detail-wrapper"
            )))

            # ğŸ‘‰ ë¬¸ë‹¨ ê°œìˆ˜ ë„ˆë¬´ ë¹¡ë¹¡í•˜ê²Œ ë³´ì§€ ë§ê³ , ì‹¤íŒ¨í•´ë„ ê·¸ëƒ¥ ì§„í–‰
            try:
                local_wait.until(
                    lambda d: len(
                        d.find_elements(
                            By.CSS_SELECTOR,
                            "section#article-detail-wrapper p.typo-contents2"
                        )
                    ) >= 1     # ìµœì†Œ 1ê°œë§Œ ë‚˜ì˜¤ë©´ í†µê³¼
                )
            except TimeoutException:
                # ë¬¸ë‹¨ì´ ì ê±°ë‚˜ ëŠ¦ê²Œ ë– ë„ ê·¸ëƒ¥ í˜„ì¬ ìˆëŠ” ê²ƒë§Œ ê¸ê³  ë„˜ì–´ê°€ê¸°
                pass

            paragraph_els = content_section.find_elements(
            By.XPATH,
            ".//p | .//h3 | .//h4 | .//blockquote"
            )

            paragraphs = [el.text.strip() for el in paragraph_els if el.text.strip()]

            # ì„¹ì…˜ ì•ˆì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ í¬í•¨í•´ì„œ ê°€ì ¸ì˜¤ê¸°
            full_text = content_section.text.strip()
            data["content_raw"] = full_text

            data["content_paragraphs"] = paragraphs

            data["content_raw"] = "\n\n".join(paragraphs)

            return data
        try:
            article_data = parse_article_detail(driver, timeout=15)
        except TimeoutException:
            print("[WARN] ë³¸ë¬¸ ë¡œë”© ì‹¤íŒ¨, ìŠ¤í‚µ:", item["url"])
            continue
        except Exception as e:
            print("[ERROR] ì˜ˆê¸°ì¹˜ ëª»í•œ ì—ëŸ¬, ìŠ¤í‚µ:", item["url"], e)
            continue

        detail_results.append({
            "source" : item["source"],
            "externalId":item["externalId"],
            "url": item["url"],
            "title": article_data["title"],
            "reporter": article_data["author"],
            "publishedDate": article_data["posted_at"],
            "category": article_data["category"],
            "content": article_data["content_raw"],  # ë‚˜ì¤‘ì— ìš”ì•½ ëª¨ë¸ì— ë„£ì„ ì›ë¬¸
            "thumbnailUrl": item["thumbnail_url"],
            "provider" : item["provider"]
        })
    driver.quit()


    # KoBERT ìš”ì•½
    for article in detail_results:
        text = article.get("content", "")
        if not text:
            article["content"] = ""
            continue
        summary, _ = summarize_kobert(text, top_k=7)
        article["content"] = summary


    return detail_results




'''
def crawl_woowahan():
    SOURCE = "WOOWATECH" 
    url = 'https://techblog.woowahan.com/'
    HEADERS = {"User-Agent": "Mozilla/5.0"}
    provider = "woowahan"

    def parse_post_date(text: str) -> date:
        text = text.strip()
        # ê¸°ë³¸ í˜•ì‹: Dec.02.2025
        return datetime.strptime(text, "%b.%d.%Y").date()
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê°œë³„ ê¸°ì‚¬ í˜ì´ì§€ íŒŒì‹± í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def parse_article(url: str) -> dict:
        """
        ê¸€ ìƒì„¸ í˜ì´ì§€ì— ë“¤ì–´ê°€ì„œ ì¶”ê°€ ì •ë³´(ë³¸ë¬¸, íƒœê·¸ ë“±)ë¥¼ ë½‘ëŠ” ì˜ˆì‹œ.
        CSS selectorëŠ” ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •í•´ì•¼ í•¨!
        """
        res = requests.get(url, headers=HEADERS)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        # 1) ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (ì˜ˆì‹œ selector)
        #   - ì‹¤ì œë¡œëŠ” F12ë¡œ ë³´ê³  class ì´ë¦„ í™•ì¸í•´ì„œ ê³ ì³ì¤˜ì•¼ í•¨
        content_el = (
            soup.select_one("div.post-content") or
            soup.select_one("div.entry-content") or
            soup.select_one("article")
        )
        content_text = content_el.get_text("\n", strip=True) if content_el else ""
        # 1) ì‘ì„±ì + ìƒì„¸ ë‚ ì§œ
        author = None
        detail_date_raw = None
        detail_date = None

        author_box = soup.select_one("div.post-header-author")
        if author_box:
            span_texts = [
                s.get_text(strip=True)
                for s in author_box.find_all("span")
                if s.get_text(strip=True)
            ]
            if len(span_texts) >= 1:
                detail_date_raw = span_texts[0]
            if len(span_texts) >= 2:
                author = span_texts[1]

            # ë‚ ì§œë¥¼ date ê°ì²´ë¡œ íŒŒì‹± (ì‹¤íŒ¨í•˜ë©´ ê·¸ëƒ¥ rawë§Œ ë‘ )
            if detail_date_raw:
                try:
                    detail_date = datetime.strptime(detail_date_raw, "%Y. %m. %d.").date()
                except ValueError:
                    detail_date = None
        category = None
        category_slug = None
        cat_el = soup.select_one("p.post-header-categories a.cat-tag")
        if cat_el:
            category = cat_el.get_text(strip=True)       # ì˜ˆ: "Infra"
            category_slug = cat_el.get("data-slug")      # ì˜ˆ: "infra"     
 

        return {
            "source" : SOURCE,
            "content": content_text,
            "author": author,
            "category": category,
            # "tags": tags,  # í•„ìš”í•˜ë©´ ì£¼ì„ í•´ì œ + ìœ„ ì½”ë“œ í™œì„±í™”
        }
    
    # 1) ì–´ì œ ë‚ ì§œ ë¬¸ìì—´ ë§Œë“¤ê¸°
    yesterday = date.today() - timedelta(days=1)# ìˆ˜ì •ì‚¬í•­!! 
    #print(yesterday)
    
    #target_date_str = yesterday.strftime("%Y. %m. %d.")  # "2025. 12. 02." í˜•íƒœ
    target_date_str=str(yesterday)
    #print(type(yesterday))
    res = requests.get(url, headers=HEADERS)
    res.raise_for_status()

    soup = BeautifulSoup(res.text, "html.parser")

    posts = []

    # 2) ëª¨ë“  ê²Œì‹œê¸€ ì¹´ë“œ ì°¾ê¸°
    for item in soup.select("div.post-item"):
        time_tag = item.select_one("time.post-author-date")
        if not time_tag:
            continue

        # ex) "2025. 12. 02."
        date_text = time_tag.get_text(strip=True)
        if len(date_text) == 0:
            continue
        #print(date_text)
        post_date = str(parse_post_date(date_text))
        # 3) ë‚ ì§œê°€ ì–´ì œì™€ ê°™ì€ ì¹´ë“œë§Œ í†µê³¼

        if post_date != target_date_str:
            #print(f"post : {post_date}")
            #print(f"target :{target_date_str}")
            continue


        # 4) ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
        title_tag = item.select_one("h2.post-title")

        if not title_tag:
            continue

        title = title_tag.get_text(strip=True)


        # ì œëª©ì„ ê°ì‹¸ê³  ìˆëŠ” <a> (ê¸°ì‚¬ ë§í¬)
        link_tag = title_tag.find_parent("a")
        if not link_tag:
            continue

        url = link_tag["href"]

        article_info = parse_article(url) 
        #print(article_info)

        summary, summary_sents = summarize_kobert(article_info["content"], top_k=7)
        posts.append({
            "source" : SOURCE,
            "externalid": None,
            "provider" : provider,
            "publishedDate": yesterday,
            "title": title,
            "url": url,
            "category": article_info["category"],
            "content": summary,#summary,
            "reporter": article_info["author"],
            "thumbnailUrl": None, 
        })

    return posts
'''

if __name__ == "__main__":
    data = crawl_news()
    # data2 = crawl_woowahan()
    data_f = data # +data2
    df = pd.DataFrame(data_f)
    df.to_json(
        "news_output.json",
        orient="records",
        force_ascii=False,
        indent=2,
    )
    print("news_output.json ì €ì¥ ì™„ë£Œ!")